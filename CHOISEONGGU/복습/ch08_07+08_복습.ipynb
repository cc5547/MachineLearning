{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmqou2s5PUUX3UeXg9Apld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHOISEONGGU/MachineLearning_ChoiSG/blob/main/CHOISEONGGU/%EB%B3%B5%EC%8A%B5/ch08_07%2B08_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost\n",
        "\n",
        "* 결정 트리 (의사결정나무) : 특정한 변수를 기준으로 두 그룹으로 나눠서, 예측값을 맞추는 알고리즘\n",
        "   * 부분적인 데이터에 너무 특화되어서 과최적화가 일어날 수 있다.\n",
        "* (배깅-부트스트랩) : 부분적인 데이터 행을 가지고 무작위 & 복원 추출 -> 부분집합 -> 과최적화\n",
        "* 랜덤 포레스트 : 데이터 행 + 데이터 열 -> 부분집합을 만들자\n",
        "(배깅, 랜덤 포레스트 -> 각 트리들이 독립적 -> 각각 만들어질 떄 서로에게 영향(X)\n",
        "---\n",
        "* 부스팅 : 각각 이전에 만들어진 트리가 이후 만들어진 트리에 영향을 줘서, 좀 더 정확하게 & 더 빠르게 만들 수 있는 성능 개선 기법\n",
        "  * adaboost : 잘 분류 안되는 데이터들한테 가중치를 줘서 이걸 좀 더 집중적으로 분류할 수 있게..\n",
        "* 경사 부스팅\n",
        "  * 경사하강법 -> 오류 -> 오류 함수, 손실 함수 -> 기울기를 최적화 방식으로 -> 오류를 줄이는 방식\n",
        "  * XGBoost, LightGBM, Catboost\n"
      ],
      "metadata": {
        "id": "OH5ZpvPV97ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM\n",
        "\n",
        "* 리프 중심 트리(노드) 분할 기법을 사용한다.\n",
        "* 트리 - 루트(부모) 노드 -> 리프(자식) 노드 (<-> XGBoost : 균형분할방식 - 좌우 노드 수가 균등할 수 있게 반반 나뉘는 것으로 먼저 처리를 한다.)\n",
        "* 이진 분류 -> 1, 0 => 몰아줄 수 있으면 먼저 처리르 해서 해당 노드는 더 이상 진행하지 않는다.\n",
        "* 장점 : 빠르고, 성능이 높게 나온다.\n",
        "* 단점 : 과최적화..# LightGBM\n",
        "\n",
        "* 리프 중심 트리(노드) 분할 기법을 사용한다.\n",
        "* 트리 - 루트(부모) 노드 -> 리프(자식) 노드 (<-> XGBoost : 균형분할방식 - 좌우 노드 수가 균등할 수 있게 반반 나뉘는 것으로 먼저 처리를 한다.)\n",
        "* 이진 분류 -> 1, 0 => 몰아줄 수 있으면 먼저 처리르 해서 해당 노드는 더 이상 진행하지 않는다.\n",
        "* 장점 : 빠르고, 성능이 높게 나온다.\n",
        "* 단점 : 과최적화.."
      ],
      "metadata": {
        "id": "bIWkQSo6_6GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 과정에 관한 설명"
      ],
      "metadata": {
        "id": "9rpdQhGsADnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 피쳐 엔지니어링(특성 공학)\n",
        "* 피쳐 - 독립변수 - 관측값 - X\n",
        "* 어떠한 변수를 선택할 것이냐? 어떠한 변수를 만들것이냐?\n",
        "* 변수를 만들어서 영향?\n",
        "* 인종의 유사 여부? -> 도메인 지식, 통계적으로, (z-점수, 차이값)\n",
        "* 행, 열을 가지고 조작하는 apply, 그룹을 만들어 연산 groupby, agg"
      ],
      "metadata": {
        "id": "X7LoDRbk__5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 분류 평가 ( classifier )\n",
        "\n",
        "## 정확도, 일치도 : accuracy_score\n",
        "  * ( TP + TN / (TP + FP + TN + NP )\n",
        "  * 모델을 통해서 예측한 결과값 vs 실제 목표값 사이의 일치 여부\n",
        "  * 0~1 사이의 일지 정도, 비율\n",
        "* 원래 99%로 한 쪽에 만약 기울어 있는 데이터라면?\n",
        "* 90%로 잘 맞추는데, 0을 엄청 못 맞춘다면? (1종 오류 or 2종 오류 같은거?)\n",
        "* 점수는 높은데 뭔가 계속 놓쳐? vs 상대적으로 0.1점 같이 점수는 낮은데... 균형있게 잘 분류해?\n",
        "\n",
        "## 혼동 행렬(confusion  matrix)\n",
        "\n",
        "* 맞다 : ( 1 positive, 0 negative )\n",
        "\n",
        "|실제|예측|명칭|\n",
        "|-|-|-|\n",
        "|1|1|TP|\n",
        "|0|0|TN|\n",
        "|1|0|FN (위음성 - 2종 오류)|\n",
        "|0|1|FP (위양성 - 1종 오류)|\n",
        "\n",
        "## 분류 리포트 (classification report)\n",
        "\n",
        "* 정밀도 (Precision) : TP / (TP+FP)\n",
        "  * 양성으로 분류한 것 중에서 진짜 양성은 얼마 정도인가? (잘 맞춘다는거지요?)\n",
        "  * 위양성(잘못 양성으로 분류한 것)이 얼마나 적은가?\n",
        "* 재현율 (recall) : TP / (TP+FN)\n",
        "  * 다시 했을 때 같은 결과가 나오는 것인가\n",
        "  * 위음성 (잘못 음성으로 분류한 것)이 얼마나 적은가?\n",
        "* F-1 점수(f1-score)\n",
        "  * 정밀도와 재현율의 조화 평균\n",
        "  * 둘 다 신경써야할 때\n",
        "\n",
        "$2 \\times \\frac{precision \\times recall}{precesion + recall}$\n",
        "=\n",
        "$2 \\times \\frac{정밀도 \\times 재현율}{정밀도 + 재현율}$\n",
        "\n",
        "\n",
        "  `from sklearn.metrics import classification_report`"
      ],
      "metadata": {
        "id": "aDGmw4sMBTMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 하이퍼 패러미터 튜닝\n",
        "* 학습률(learning rate) : 경사하강법을 쓸 때 얼마나 민감하게(세세하게) 조정을 할 것이냐? -> 민감할 수록 오래걸리고, 과최적화. 둔감할 수록 학습 자체가 덜 될 가능성이 있음\n",
        "* 부분샘플(subsample) : 부트스트랩, 배깅... -> 부분집합을 만들 때 얼마나 비율을 뽑아서 할 것이냐?\n",
        "* 그리트 서치 : 몇몇 대표적으로 쓰이는 설정값들을 다 조합을 넣어놓고, 하나하나 격자(조합)를 모델로 만들어서 평가 지표로 비교하는 기법\n",
        "* 장점\n",
        "  * 내가 모델을 하나씩 안 만들어 보아도 가장 최고의 조합을 찾아줌\n",
        "  * cv(cross-validation/k-fold) 까지 하면 모델 수백개 만들어야 함 -> 아무리 빨라도 몇 시간 걸림\n",
        "---\n",
        "* 피쳐 셀렉션\n",
        "  * `model.feature_importance_`\n",
        "  * 가장 많이 선택된(영향을 미친) 변수들을 순서대로 볼 수 있다.\n",
        "  * 주요 변수만 선택해서, 다시 변수 추려내서 돌려볼 수 있음\n",
        "  * 주요 변수 -> 새로운 파생 변수."
      ],
      "metadata": {
        "id": "ZiGwYtkIFD_6"
      }
    }
  ]
}